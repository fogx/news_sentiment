{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkVsHkI6EWEh"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-07Z6JhMWQXS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9dde95ce-a4c8-4ac8-c238-eb1b81770996"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 3.4 MB 14.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 3.3 MB 24.2 MB/s \n",
      "\u001B[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
      "\u001B[K     |████████████████████████████████| 895 kB 44.5 MB/s \n",
      "\u001B[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 306 kB 29.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 51.5 MB/s \n",
      "\u001B[K     |████████████████████████████████| 243 kB 73.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 133 kB 67.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 144 kB 67.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 271 kB 70.9 MB/s \n",
      "\u001B[K     |████████████████████████████████| 160 kB 70.2 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 21.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.2 MB 28.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.7 MB 29.3 MB/s \n",
      "\u001B[K     |████████████████████████████████| 180 kB 53.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 97 kB 6.8 MB/s \n",
      "\u001B[K     |████████████████████████████████| 142 kB 55.0 MB/s \n",
      "\u001B[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
      "\u001B[?25h  Building wheel for subprocess32 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install sklearn --quiet\n",
    "!pip install tensorflow-addons --quiet\n",
    "!pip install germansentiment --quiet\n",
    "!pip install transformers[sentencepiece] --quiet\n",
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLbCb71GWtJS"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification,\n",
    "    AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from transformers import create_optimizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "from germansentiment import SentimentModel\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import sklearn\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Zo5ZyrE57j"
   },
   "outputs": [],
   "source": [
    "# To use the data I import it as a pandas data frame\n",
    "df = pd.read_csv('df_wirtschaft_labeled.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA6YqH9wZ1BJ"
   },
   "outputs": [],
   "source": [
    "df.hist(column=[\"label_title\", \"label_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# create test lists for title and body\n",
    "input_titles_raw = [x for x in df[\"title\"]]\n",
    "print(input_titles_raw[:3])\n",
    "input_bodies_raw = [x for x in df[\"body_512\"]]\n",
    "print(input_bodies_raw[:3])"
   ],
   "metadata": {
    "id": "JoMEqlKplfEg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create english test lists for title and body\n",
    "# helsinki-nlp opus\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "%time\n",
    "input_titles_en_raw =[x[\"translation_text\"] for x in translator(input_titles_raw)]\n",
    "%time\n",
    "input_bodies_en_raw =[x[\"translation_text\"] for x in translator(input_bodies_raw)]"
   ],
   "metadata": {
    "id": "cJiU3br1Om2m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# german-sentiment-bert\n",
    "# https://huggingface.co/oliverguhr/german-sentiment-bert\n",
    "model = SentimentModel()\n",
    "%time\n",
    "df[\"gsb_title\"] = model.predict_sentiment(input_titles_raw)\n",
    "%time\n",
    "df[\"gsb_body\"]  = model.predict_sentiment(input_bodies_raw)\n",
    "df[\"gsb_title\"] = df[\"gsb_title\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                        \"1\").astype(int)\n",
    "df[\"gsb_body\"] = df[\"gsb_body\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                      \"1\").astype(int)"
   ],
   "metadata": {
    "id": "qBMYX1Axxd4w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/mdraw/german-news-sentiment-bert\n",
    "model_f = SentimentModel('mdraw/german-news-sentiment-bert')\n",
    "%time df[\"gsb_f_title\"] = model_f.predict_sentiment(input_titles_raw)\n",
    "%time\n",
    "df[\"gsb_f_body\"] = model_f.predict_sentiment(input_bodies_raw)\n",
    "df[\"gsb_f_title\"] = df[\"gsb_f_title\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                            \"1\").astype(\n",
    "    int)\n",
    "df[\"gsb_f_body\"] = df[\"gsb_f_body\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                          \"1\").astype(\n",
    "    int)"
   ],
   "metadata": {
    "id": "hEUWjovYK7fB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def nlptown(input_raw):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 2 else 0 if index_max < 3 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"nlptown_bert_title\"]  = nlptown(input_titles_raw)\n",
    "split_bodies = [input_bodies_raw[i:i + 20] for i in range(0, len(input_bodies_raw), 20)]\n",
    "%time\n",
    "df[\"nlptown_bert_body\"]  =[item for sublist in split_bodies for item in nlptown(sublist)]"
   ],
   "metadata": {
    "id": "YX4qVJSsCg72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "id": "_a-H_c2KT6Nm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/deepset/bert-base-german-cased-sentiment-Germeval17\n",
    "tokenizer_deepset = AutoTokenizer.from_pretrained(\"deepset/bert-base-german-cased-sentiment-Germeval17\")\n",
    "model_deepset = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"deepset/bert-base-german-cased-sentiment-Germeval17\")\n",
    "\n",
    "\n",
    "def deepset(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"deepset_title\"]  = deepset(input_titles_raw, tokenizer_deepset, model_deepset)\n",
    "split_bodies = [input_bodies_raw[i:i + 20] for i in range(0, len(input_bodies_raw), 20)]\n",
    "%time\n",
    "df[\"deepset_body\"]  =[item for sublist in split_bodies for item in deepset(sublist, tokenizer_deepset, model_deepset)]"
   ],
   "metadata": {
    "id": "62swkGTg4WxW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# roberta_en_3_classes\n",
    "t_r = AutoTokenizer.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "m_r = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "\n",
    "\n",
    "def roberta_en(input_raw, tokenizer, model):\n",
    "    # predict on translation\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"roberta_en_title\"]  = roberta_en(input_titles_en_raw, t_r, m_r)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"roberta_en_body\"]  =[item for sublist in split_bodies for item in roberta_en(sublist, t_r, m_r)]"
   ],
   "metadata": {
    "id": "svhuXu668mln"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert \n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# helsinki-nlp opus\n",
    "# translate data to german\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "translator_en_de = pipeline(\"translation\", model=model_checkpoint)"
   ],
   "metadata": {
    "id": "tFioIytJfw2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_untrained\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "t_d = AutoTokenizer.from_pretrained(checkpoint)\n",
    "m_d = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def distilbert_en(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"distilbert_untrained_en_title\"]  = distilbert_en(input_titles_en_raw, t_d, m_d)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_untrained_en_body\"]  =[item for sublist in split_bodies for item in distilbert_en(sublist, t_d, m_d)]"
   ],
   "metadata": {
    "id": "O52VHyQ7MPAv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_untrained WITH GERMAN TEXT\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "t_d = AutoTokenizer.from_pretrained(checkpoint)\n",
    "m_d = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def distilbert_en(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"distilbert_untrained_title\"]  = distilbert_en(input_titles_raw, t_d, m_d)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_untrained_body\"]  =[item for sublist in split_bodies for item in distilbert_en(sublist, t_d, m_d)]"
   ],
   "metadata": {
    "id": "7FuEBjLjVjw5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# To use the data I import it as a pandas data frame\n",
    "df_train = pd.read_csv('all-data.csv', encoding=\"ISO-8859-1\")\n",
    "df_train.head(5)"
   ],
   "metadata": {
    "id": "wI71Ng_V_v1P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# turn labels into \n",
    "df_train[\"label\"] = df_train[\"label\"].str.replace(\"negative\", \"0\").str.replace(\"neutral\", \"1\").str.replace(\"positive\",\"2\").astype(int)"
   ],
   "metadata": {
    "id": "cVvF5rAFOJ63"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create arrow dataset from pandas\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "# split arrow dataset into training and validation \n",
    "dataset_split = dataset.train_test_split(test_size=0.1)\n",
    "train_data = dataset_split['train']\n",
    "validation_data = dataset_split['test']\n",
    "# create dataset dict which holds both datasets and allows map() to run on both\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': validation_data\n",
    "})\n",
    "# use sklearn to calculate classweights\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes=np.unique(df_train['label']),y=df_train['label'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"class_weights:\", class_weights)\n"
   ],
   "metadata": {
    "id": "7SATe9LPEluR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenize arrow dataset-dict\n",
    "def tokenize_function(dat):\n",
    "    return tokenizer(dat[\"headline\"], truncation=True)\n",
    "\n",
    "\n",
    "dataset_t = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
   ],
   "metadata": {
    "id": "ZIrBEEdNoY0I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tf_train_dataset = dataset_t[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = dataset_t[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "id": "akwQAbygqPC0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "init_lr = 2e-5\n",
    "batches_per_epoch = len(dataset_t[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=init_lr, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "# define model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "model.compile(optimizer=optimizer)"
   ],
   "metadata": {
    "id": "QiZWbpU8iAVR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.init(project=\"news-sentiment\", entity=\"fogx\", config={\"batch_size\": batch_size, \"lr\": 2e-5})\n",
    "callbacks = [WandbCallback()]\n",
    "name_model = 'Distilbert'"
   ],
   "metadata": {
    "id": "hjbym-Awr8U_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    class_weight=class_weights\n",
    ")"
   ],
   "metadata": {
    "id": "JELlLg-nqgid"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dir = os.path.join(os.getcwd(), 'checkpoints_distilbert', \"model_best_lr_2e5\")\n",
    "model.save_pretrained(dir)"
   ],
   "metadata": {
    "id": "0RPO8RsayT7V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t = tokenizer(input_titles_en_raw[:3], padding=True, truncation=True, return_tensors=\"tf\")\n",
    "o = model(t)\n",
    "p = tf.math.softmax(o.logits, axis=-1)\n",
    "print(p)"
   ],
   "metadata": {
    "id": "3tcZVJy65Cl_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_trained\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "\n",
    "def distilbert_trained(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "\n",
    "distilbert_trained(input_titles_raw[:3], tokenizer, model)\n",
    "\n",
    "%time\n",
    "df[\"distilbert_trained_en_lr2_title\"]  = distilbert_trained(input_titles_en_raw, tokenizer, model)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_trained_en_lr2_body\"]  =[item for sublist in split_bodies for item in distilbert_trained(sublist, tokenizer, model)]\n",
    "print(\"with german texts\")\n",
    "%time\n",
    "df[\"distilbert_trained_lr2_title\"]  = distilbert_trained(input_titles_raw, tokenizer, model)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_trained_lr2_body\"]  =[item for sublist in split_bodies for item in distilbert_trained(sublist, tokenizer, model)]"
   ],
   "metadata": {
    "id": "RrCCwr1axsg2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluation\n",
    "columns = [\"gsb\", \"gsb_f\", \"nlptown_bert\", \"deepset\", \"roberta_en\", \"distilbert_untrained_en\", \"distilbert_untrained\",\n",
    "           \"distilbert_trained_en\", \"distilbert_trained\", \"distilbert_trained_lr2\"]\n",
    "cms = {}\n",
    "for col in columns:\n",
    "    col_title = f\"{col}_title\"\n",
    "    col_body = f\"{col}_body\"\n",
    "    title_correct = len(df[df[\"label_title\"] == df[col_title]])\n",
    "    body_correct = len(df[df[\"label_body\"] == df[col_body]])\n",
    "    df_len = len(df)\n",
    "    print(f'correct in {col_title}: {title_correct}/{df_len} -> {round(title_correct / df_len, 2) * 100}%')\n",
    "    print(f'correct in {col_body}: {body_correct}/{df_len} -> {round(body_correct / df_len, 2) * 100}%')\n",
    "    #confusion matrices\n",
    "    cm_t = confusion_matrix(df[\"label_title\"], df[col_title])\n",
    "    cm_b = confusion_matrix(df[\"label_body\"], df[col_body])\n",
    "    cms[col] = [cm_t, cm_b]"
   ],
   "metadata": {
    "id": "x8zaF8dc2-2k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cms[\"gsb\"]"
   ],
   "metadata": {
    "id": "MEMzC6VZz910"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ax = sns.heatmap(cms[\"distilbert_trained_en\"][0], annot=True, cmap='Blues')\n",
    "ax.set_title('Confusion matrix')\n",
    "ax.set_xlabel('Predicted values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "ax.xaxis.set_ticklabels(['positive', 'neutral', 'negative'])\n",
    "ax.yaxis.set_ticklabels(['positive', 'neutral', 'negative'])"
   ],
   "metadata": {
    "id": "sNPoU6u0vved"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis_BERT-newspaper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}