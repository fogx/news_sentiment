{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkVsHkI6EWEh"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-07Z6JhMWQXS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9dde95ce-a4c8-4ac8-c238-eb1b81770996"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 3.4 MB 14.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 3.3 MB 24.2 MB/s \n",
      "\u001B[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
      "\u001B[K     |████████████████████████████████| 895 kB 44.5 MB/s \n",
      "\u001B[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 306 kB 29.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 51.5 MB/s \n",
      "\u001B[K     |████████████████████████████████| 243 kB 73.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 133 kB 67.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 144 kB 67.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 271 kB 70.9 MB/s \n",
      "\u001B[K     |████████████████████████████████| 160 kB 70.2 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 21.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.2 MB 28.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.7 MB 29.3 MB/s \n",
      "\u001B[K     |████████████████████████████████| 180 kB 53.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 97 kB 6.8 MB/s \n",
      "\u001B[K     |████████████████████████████████| 142 kB 55.0 MB/s \n",
      "\u001B[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
      "\u001B[?25h  Building wheel for subprocess32 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install sklearn --quiet\n",
    "!pip install tensorflow-addons --quiet\n",
    "!pip install germansentiment --quiet\n",
    "!pip install transformers[sentencepiece] --quiet\n",
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLbCb71GWtJS"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification,AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from transformers import create_optimizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "from germansentiment import SentimentModel\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import sklearn\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l4Zo5ZyrE57j"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'df_wirtschaft_labeled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# To use the data I import it as a pandas data frame\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdf_wirtschaft_labeled.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    666\u001B[0m     dialect,\n\u001B[0;32m    667\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    677\u001B[0m )\n\u001B[0;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1213\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[0;32m   1216\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[1;32m-> 1217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[0;32m   1218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1228\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\pandas\\io\\common.py:789\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    785\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    787\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    788\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'df_wirtschaft_labeled.csv'"
     ]
    }
   ],
   "source": [
    "# To use the data I import it as a pandas data frame\n",
    "df = pd.read_csv('df_wirtschaft_labeled.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA6YqH9wZ1BJ"
   },
   "outputs": [],
   "source": [
    "df.hist(column=[\"label_title\", \"label_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# create test lists for title and body\n",
    "input_titles_raw = [x for x in df[\"title\"]]\n",
    "print(input_titles_raw[:3])\n",
    "input_bodies_raw = [x for x in df[\"body_512\"]]\n",
    "print(input_bodies_raw[:3])"
   ],
   "metadata": {
    "id": "JoMEqlKplfEg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create english test lists for title and body\n",
    "# helsinki-nlp opus\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "%time\n",
    "input_titles_en_raw =[x[\"translation_text\"] for x in translator(input_titles_raw)]\n",
    "%time\n",
    "input_bodies_en_raw =[x[\"translation_text\"] for x in translator(input_bodies_raw)]"
   ],
   "metadata": {
    "id": "cJiU3br1Om2m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# german-sentiment-bert\n",
    "# https://huggingface.co/oliverguhr/german-sentiment-bert\n",
    "model = SentimentModel()\n",
    "%time\n",
    "df[\"gsb_title\"] = model.predict_sentiment(input_titles_raw)\n",
    "%time\n",
    "df[\"gsb_body\"]  = model.predict_sentiment(input_bodies_raw)\n",
    "df[\"gsb_title\"] = df[\"gsb_title\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                        \"1\").astype(int)\n",
    "df[\"gsb_body\"] = df[\"gsb_body\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                      \"1\").astype(int)"
   ],
   "metadata": {
    "id": "qBMYX1Axxd4w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/mdraw/german-news-sentiment-bert\n",
    "model_f = SentimentModel('mdraw/german-news-sentiment-bert')\n",
    "%time df[\"gsb_f_title\"] = model_f.predict_sentiment(input_titles_raw)\n",
    "%time\n",
    "df[\"gsb_f_body\"] = model_f.predict_sentiment(input_bodies_raw)\n",
    "df[\"gsb_f_title\"] = df[\"gsb_f_title\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                            \"1\").astype(\n",
    "    int)\n",
    "df[\"gsb_f_body\"] = df[\"gsb_f_body\"].str.replace(\"negative\", \"-1\").str.replace(\"neutral\", \"0\").str.replace(\"positive\",\n",
    "                                                                                                          \"1\").astype(\n",
    "    int)"
   ],
   "metadata": {
    "id": "hEUWjovYK7fB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def nlptown(input_raw):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 2 else 0 if index_max < 3 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"nlptown_bert_title\"]  = nlptown(input_titles_raw)\n",
    "split_bodies = [input_bodies_raw[i:i + 20] for i in range(0, len(input_bodies_raw), 20)]\n",
    "%time\n",
    "df[\"nlptown_bert_body\"]  =[item for sublist in split_bodies for item in nlptown(sublist)]"
   ],
   "metadata": {
    "id": "YX4qVJSsCg72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "id": "_a-H_c2KT6Nm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# https://huggingface.co/deepset/bert-base-german-cased-sentiment-Germeval17\n",
    "tokenizer_deepset = AutoTokenizer.from_pretrained(\"deepset/bert-base-german-cased-sentiment-Germeval17\")\n",
    "model_deepset = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"deepset/bert-base-german-cased-sentiment-Germeval17\")\n",
    "\n",
    "\n",
    "def deepset(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"deepset_title\"]  = deepset(input_titles_raw, tokenizer_deepset, model_deepset)\n",
    "split_bodies = [input_bodies_raw[i:i + 20] for i in range(0, len(input_bodies_raw), 20)]\n",
    "%time\n",
    "df[\"deepset_body\"]  =[item for sublist in split_bodies for item in deepset(sublist, tokenizer_deepset, model_deepset)]"
   ],
   "metadata": {
    "id": "62swkGTg4WxW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# roberta_en_3_classes\n",
    "t_r = AutoTokenizer.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "m_r = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "\n",
    "\n",
    "def roberta_en(input_raw, tokenizer, model):\n",
    "    # predict on translation\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**input)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"roberta_en_title\"]  = roberta_en(input_titles_en_raw, t_r, m_r)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"roberta_en_body\"]  =[item for sublist in split_bodies for item in roberta_en(sublist, t_r, m_r)]"
   ],
   "metadata": {
    "id": "svhuXu668mln"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert \n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# helsinki-nlp opus\n",
    "# translate data to german\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "translator_en_de = pipeline(\"translation\", model=model_checkpoint)"
   ],
   "metadata": {
    "id": "tFioIytJfw2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_untrained\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "t_d = AutoTokenizer.from_pretrained(checkpoint)\n",
    "m_d = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def distilbert_en(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"distilbert_untrained_en_title\"]  = distilbert_en(input_titles_en_raw, t_d, m_d)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_untrained_en_body\"]  =[item for sublist in split_bodies for item in distilbert_en(sublist, t_d, m_d)]"
   ],
   "metadata": {
    "id": "O52VHyQ7MPAv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_untrained WITH GERMAN TEXT\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "t_d = AutoTokenizer.from_pretrained(checkpoint)\n",
    "m_d = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def distilbert_en(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "%time\n",
    "df[\"distilbert_untrained_title\"]  = distilbert_en(input_titles_raw, t_d, m_d)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_untrained_body\"]  =[item for sublist in split_bodies for item in distilbert_en(sublist, t_d, m_d)]"
   ],
   "metadata": {
    "id": "7FuEBjLjVjw5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('C:/Users/LeonWolf/Private/Projects/Python/news_sentiment')"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import get_project_root\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# To use the data I import it as a pandas data frame\n",
    "df_train = pd.read_csv(get_project_root() / \"data/raw\" /'all-data.csv', encoding=\"ISO-8859-1\")\n",
    "df_train.head(5)"
   ],
   "metadata": {
    "id": "wI71Ng_V_v1P"
   },
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                           headline\n0   neutral  According to Gran , the company has no plans t...\n1   neutral  Technopolis plans to develop in stages an area...\n2  negative  The international electronic industry company ...\n3  positive  With the new production plant the company woul...\n4  positive  According to the company 's updated strategy f...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>headline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>According to Gran , the company has no plans t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>Technopolis plans to develop in stages an area...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>The international electronic industry company ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>With the new production plant the company woul...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>According to the company 's updated strategy f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# turn labels into \n",
    "df_train[\"label\"] = df_train[\"label\"].str.replace(\"negative\", \"0\").str.replace(\"neutral\", \"1\").str.replace(\"positive\",\"2\").astype(int)"
   ],
   "metadata": {
    "id": "cVvF5rAFOJ63"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create arrow dataset from pandas\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "# split arrow dataset into training and validation \n",
    "dataset_split = dataset.train_test_split(test_size=0.1)\n",
    "train_data = dataset_split['train']\n",
    "validation_data = dataset_split['test']\n",
    "# create dataset dict which holds both datasets and allows map() to run on both\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': validation_data\n",
    "})\n",
    "# use sklearn to calculate classweights\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes=np.unique(df_train['label']),y=df_train['label'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"class_weights:\", class_weights)\n"
   ],
   "metadata": {
    "id": "7SATe9LPEluR"
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights: {0: 2.6743929359823397, 1: 0.5610744471459997, 2: 1.1851308388359012}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenize arrow dataset-dict\n",
    "def tokenize_function(dat):\n",
    "    return tokenizer(dat[\"headline\"], truncation=True)\n",
    "\n",
    "\n",
    "dataset_t = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
   ],
   "metadata": {
    "id": "ZIrBEEdNoY0I"
   },
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1058813b32e4819aa247991de5a4936"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45e4df72837b42d483eacf0407984541"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a49ee2d21b8949098577bc3e1d5ba1d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1543d63eb18475f9c998c67bf8be99c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3280ac8cb6f14dcc990f9216b962bacb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tf_train_dataset = dataset_t[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = dataset_t[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "id": "akwQAbygqPC0"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "# define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "init_lr = 2e-5\n",
    "# batches_per_epoch = len(dataset_t[\"train\"]) // batch_size\n",
    "batches_per_epoch = 10\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=init_lr, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "# define model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "model.compile(optimizer=optimizer)"
   ],
   "metadata": {
    "id": "QiZWbpU8iAVR"
   },
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as the 'labels' key of the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "{'label': 2,\n 'headline': 'Cargo volume increased by approximately 5 % .',\n 'input_ids': [101, 6636, 3872, 3445, 2011, 3155, 1019, 1003, 1012, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_t[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.00936219, -0.13684033,  0.00530987]], dtype=float32)>, hidden_states=None, attentions=None)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(dataset_t[\"train\"][0][\"headline\"], padding=True, truncation=True, return_tensors=\"tf\")\n",
    "model(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.init(project=\"news-sentiment\", entity=\"fogx\", config={\"batch_size\": batch_size, \"lr\": 2e-5})\n",
    "callbacks = [WandbCallback()]\n",
    "name_model = 'Distilbert'"
   ],
   "metadata": {
    "id": "hjbym-Awr8U_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_train_dataset,\n",
    "    epochs=1,\n",
    "    class_weight=class_weights\n",
    ")"
   ],
   "metadata": {
    "id": "JELlLg-nqgid"
   },
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LeonWolf\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\213.6461.77\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_vars.py\", line 456, in change_attr_expression\n",
      "    value = frame.f_locals.get(expression) or frame.f_globals.get(expression) or eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: 'LossesContainer' object has no attribute 'compiled_loss'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [58]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtf_train_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_train_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weights\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:167\u001B[0m, in \u001B[0;36mpatch_tf_keras.<locals>.new_v2\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cbk \u001B[38;5;129;01min\u001B[39;00m cbks:\n\u001B[0;32m    166\u001B[0m         set_wandb_attrs(cbk, val_data)\n\u001B[1;32m--> 167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m old_v2(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\keras\\engine\\training.py:1216\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1209\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1210\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1211\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1212\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1213\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1214\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1215\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1216\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1217\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1218\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    907\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 910\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    912\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    913\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:975\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    971\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall through to cond-based initialization.\u001B[39;00m\n\u001B[0;32m    972\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    973\u001B[0m     \u001B[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001B[39;00m\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;66;03m# stateless function.\u001B[39;00m\n\u001B[1;32m--> 975\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m   _, _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m    978\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    979\u001B[0m           \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3129\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3126\u001B[0m \u001B[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[39;00m\n\u001B[0;32m   3127\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   3128\u001B[0m   (graph_function,\n\u001B[1;32m-> 3129\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\u001B[38;5;241m.\u001B[39m_call_flat(\n\u001B[0;32m   3131\u001B[0m     filtered_flat_args, captured_inputs\u001B[38;5;241m=\u001B[39mgraph_function\u001B[38;5;241m.\u001B[39mcaptured_inputs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3557\u001B[0m, in \u001B[0;36mFunction._maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3553\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_define_function_with_shape_relaxation(\n\u001B[0;32m   3554\u001B[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001B[0;32m   3556\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_cache\u001B[38;5;241m.\u001B[39mmissed\u001B[38;5;241m.\u001B[39madd(call_context_key)\n\u001B[1;32m-> 3557\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_graph_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3558\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_cache\u001B[38;5;241m.\u001B[39mprimary[cache_key] \u001B[38;5;241m=\u001B[39m graph_function\n\u001B[0;32m   3560\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function, filtered_flat_args\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3392\u001B[0m, in \u001B[0;36mFunction._create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3387\u001B[0m missing_arg_names \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3388\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (arg, i) \u001B[38;5;28;01mfor\u001B[39;00m i, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(missing_arg_names)\n\u001B[0;32m   3389\u001B[0m ]\n\u001B[0;32m   3390\u001B[0m arg_names \u001B[38;5;241m=\u001B[39m base_arg_names \u001B[38;5;241m+\u001B[39m missing_arg_names\n\u001B[0;32m   3391\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m ConcreteFunction(\n\u001B[1;32m-> 3392\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3393\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3394\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3395\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3396\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3397\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_signature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3398\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3399\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3400\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3401\u001B[0m \u001B[43m        \u001B[49m\u001B[43moverride_flat_arg_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverride_flat_arg_shapes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m   3403\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[0;32m   3404\u001B[0m     function_spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[0;32m   3405\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[0;32m   3406\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[0;32m   3407\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[0;32m   3408\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[0;32m   3409\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   3410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1143\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1141\u001B[0m   _, original_func \u001B[38;5;241m=\u001B[39m tf_decorator\u001B[38;5;241m.\u001B[39munwrap(python_func)\n\u001B[1;32m-> 1143\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m python_func(\u001B[38;5;241m*\u001B[39mfunc_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfunc_kwargs)\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;66;03m# TensorArrays and `None`s.\u001B[39;00m\n\u001B[0;32m   1147\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m nest\u001B[38;5;241m.\u001B[39mmap_structure(convert, func_outputs,\n\u001B[0;32m   1148\u001B[0m                                   expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:672\u001B[0m, in \u001B[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m default_graph\u001B[38;5;241m.\u001B[39m_variable_creator_scope(scope, priority\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m):  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    669\u001B[0m   \u001B[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001B[39;00m\n\u001B[0;32m    670\u001B[0m   \u001B[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001B[39;00m\n\u001B[0;32m    671\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(compile_with_xla):\n\u001B[1;32m--> 672\u001B[0m     out \u001B[38;5;241m=\u001B[39m weak_wrapped_fn()\u001B[38;5;241m.\u001B[39m__wrapped__(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    673\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1118\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1116\u001B[0m \u001B[38;5;66;03m# TODO(mdan): Push this block higher in tf.function's call stack.\u001B[39;00m\n\u001B[0;32m   1117\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1118\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mautograph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1119\u001B[0m \u001B[43m      \u001B[49m\u001B[43moriginal_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1120\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1121\u001B[0m \u001B[43m      \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1122\u001B[0m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConversionOptions\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[43m          \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m          \u001B[49m\u001B[43moptional_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[43m          \u001B[49m\u001B[43muser_requested\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[43m      \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:445\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    444\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 445\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    446\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    447\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4kxq1o9m.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_function\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:337\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_in_allowlist_cache(f, options):\n\u001B[0;32m    336\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: from cache\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n\u001B[1;32m--> 337\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx()\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m ag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mDISABLED:\n\u001B[0;32m    340\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: AutoGraph is disabled in context\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:465\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    464\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 465\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\keras\\engine\\training.py:867\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function\u001B[1;34m(model, iterator)\u001B[0m\n\u001B[0;32m    864\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[0;32m    866\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[1;32m--> 867\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[0;32m    869\u001B[0m     outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    870\u001B[0m write_scalar_summaries(outputs, step\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_train_counter)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1316\u001B[0m, in \u001B[0;36mStrategyBase.run\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscope():\n\u001B[0;32m   1312\u001B[0m   \u001B[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001B[39;00m\n\u001B[0;32m   1313\u001B[0m   \u001B[38;5;66;03m# applied when the caller is also in Eager mode.\u001B[39;00m\n\u001B[0;32m   1314\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[0;32m   1315\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m-> 1316\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extended\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2892\u001B[0m, in \u001B[0;36mStrategyExtendedV1.call_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   2890\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   2891\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m-> 2892\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3695\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._call_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   3693\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_for_each_replica\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, args, kwargs):\n\u001B[0;32m   3694\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ReplicaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy(), replica_id_in_sync_group\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m-> 3695\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:696\u001B[0m, in \u001B[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    695\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m conversion_ctx:\n\u001B[1;32m--> 696\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m    698\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:383\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    380\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _call_unconverted(f, args, kwargs, options)\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39muser_requested \u001B[38;5;129;01mand\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_allowlisted(f):\n\u001B[1;32m--> 383\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001B[39;00m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001B[39;00m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001B[39;00m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;66;03m# things like builtins.\u001B[39;00m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39minternal_convert_user_code:\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    461\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__self__\u001B[39m\u001B[38;5;241m.\u001B[39mcall(args, kwargs)\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 464\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    465\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\keras\\engine\\training.py:860\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(data):\n\u001B[1;32m--> 860\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    861\u001B[0m   \u001B[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001B[39;00m\n\u001B[0;32m    862\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\transformers\\modeling_tf_utils.py:911\u001B[0m, in \u001B[0;36mTFPreTrainedModel.train_step\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m return_metrics \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m return_metrics:\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m return_metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 911\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreturn_metrics\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\news_sentiment-U8RIfLoL\\lib\\site-packages\\transformers\\modeling_tf_utils.py:911\u001B[0m, in \u001B[0;36mTFPreTrainedModel.train_step\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m return_metrics \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m return_metrics:\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m return_metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 911\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreturn_metrics\u001B[49m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1180\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:621\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1096\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1058\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:318\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\213.6461.77\\plugins\\python\\helpers\\pydev\\pydevd.py:1147\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1144\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\213.6461.77\\plugins\\python\\helpers\\pydev\\pydevd.py:1162\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1159\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1162\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1166\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dir = os.path.join(os.getcwd(), 'checkpoints_distilbert', \"model_best_lr_2e5\")\n",
    "model.save_pretrained(dir)"
   ],
   "metadata": {
    "id": "0RPO8RsayT7V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t = tokenizer(input_titles_en_raw[:3], padding=True, truncation=True, return_tensors=\"tf\")\n",
    "o = model(t)\n",
    "p = tf.math.softmax(o.logits, axis=-1)\n",
    "print(p)"
   ],
   "metadata": {
    "id": "3tcZVJy65Cl_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distilbert_trained\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "\n",
    "def distilbert_trained(input_raw, tokenizer, model):\n",
    "    input = tokenizer(input_raw, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    outputs = model(input)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    predictions_int = []\n",
    "    for pred in predictions:\n",
    "        index_max = max(range(len(pred)), key=pred.__getitem__)\n",
    "        sentiment = -1 if index_max < 1 else 0 if index_max < 2 else 1\n",
    "        predictions_int.append(sentiment)\n",
    "    return predictions_int\n",
    "\n",
    "\n",
    "distilbert_trained(input_titles_raw[:3], tokenizer, model)\n",
    "\n",
    "%time\n",
    "df[\"distilbert_trained_en_lr2_title\"]  = distilbert_trained(input_titles_en_raw, tokenizer, model)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_en_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_trained_en_lr2_body\"]  =[item for sublist in split_bodies for item in distilbert_trained(sublist, tokenizer, model)]\n",
    "print(\"with german texts\")\n",
    "%time\n",
    "df[\"distilbert_trained_lr2_title\"]  = distilbert_trained(input_titles_raw, tokenizer, model)\n",
    "chunk_size = 5\n",
    "split_bodies = [input_bodies_raw[i:i + 5] for i in range(0, len(input_bodies_raw), 5)]\n",
    "%time\n",
    "df[\"distilbert_trained_lr2_body\"]  =[item for sublist in split_bodies for item in distilbert_trained(sublist, tokenizer, model)]"
   ],
   "metadata": {
    "id": "RrCCwr1axsg2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluation\n",
    "columns = [\"gsb\", \"gsb_f\", \"nlptown_bert\", \"deepset\", \"roberta_en\", \"distilbert_untrained_en\", \"distilbert_untrained\",\n",
    "           \"distilbert_trained_en\", \"distilbert_trained\", \"distilbert_trained_lr2\"]\n",
    "cms = {}\n",
    "for col in columns:\n",
    "    col_title = f\"{col}_title\"\n",
    "    col_body = f\"{col}_body\"\n",
    "    title_correct = len(df[df[\"label_title\"] == df[col_title]])\n",
    "    body_correct = len(df[df[\"label_body\"] == df[col_body]])\n",
    "    df_len = len(df)\n",
    "    print(f'correct in {col_title}: {title_correct}/{df_len} -> {round(title_correct / df_len, 2) * 100}%')\n",
    "    print(f'correct in {col_body}: {body_correct}/{df_len} -> {round(body_correct / df_len, 2) * 100}%')\n",
    "    #confusion matrices\n",
    "    cm_t = confusion_matrix(df[\"label_title\"], df[col_title])\n",
    "    cm_b = confusion_matrix(df[\"label_body\"], df[col_body])\n",
    "    cms[col] = [cm_t, cm_b]"
   ],
   "metadata": {
    "id": "x8zaF8dc2-2k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cms[\"gsb\"]"
   ],
   "metadata": {
    "id": "MEMzC6VZz910"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ax = sns.heatmap(cms[\"distilbert_trained_en\"][0], annot=True, cmap='Blues')\n",
    "ax.set_title('Confusion matrix')\n",
    "ax.set_xlabel('Predicted values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "ax.xaxis.set_ticklabels(['positive', 'neutral', 'negative'])\n",
    "ax.yaxis.set_ticklabels(['positive', 'neutral', 'negative'])"
   ],
   "metadata": {
    "id": "sNPoU6u0vved"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis_BERT-newspaper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}